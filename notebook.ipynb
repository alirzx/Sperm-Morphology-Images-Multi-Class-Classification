{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cdae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\hub\n",
      "O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_HOME\"] = r\"O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\"\n",
    "\n",
    "import torch\n",
    "print(torch.hub.get_dir())\n",
    "print(os.environ[\"TORCH_HOME\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6172a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9432d88",
   "metadata": {},
   "source": [
    "# Sperm Morphology images dataset (SMIDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb20bd3",
   "metadata": {},
   "source": [
    "# XXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ac3cf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 14:32:14,253 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "import logging\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Handle truncated/corrupted images\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Create DataFrames\n",
    "def create_dataframes(root_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Create DataFrames for train, val, and test splits from SMIDS dataset.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory of the SMIDS dataset.\n",
    "        train_ratio, val_ratio, test_ratio (float): Split ratios.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_df, val_df, test_df, class_to_idx)\n",
    "    \"\"\"\n",
    "    valid_extensions = {'.png', '.jpg', '.jpeg', '.tif', '.bmp'}\n",
    "    data = []\n",
    "    class_names = ['Abnormal_Sperm', 'Non-Sperm', 'Normal_Sperm']\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "    # Verify directories\n",
    "    actual_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "    logger.info(f\"Directories found in {root_dir}: {actual_dirs}\")\n",
    "\n",
    "    # Collect image paths and labels\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            logger.warning(f\"Directory {class_dir} does not exist!\")\n",
    "            continue\n",
    "        label = class_to_idx[class_name]\n",
    "        for file in os.listdir(class_dir):\n",
    "            if os.path.splitext(file)[1].lower() in valid_extensions:\n",
    "                image_path = os.path.join(class_dir, file)\n",
    "                data.append({\n",
    "                    \"file_path\": image_path,\n",
    "                    \"label\": label,\n",
    "                    \"class_name\": class_name\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty:\n",
    "        logger.error(\"No images found in dataset!\")\n",
    "        raise ValueError(\"No valid images found.\")\n",
    "\n",
    "    # Stratified split\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, train_size=train_ratio, stratify=df['label'], random_state=42\n",
    "    )\n",
    "    val_ratio_adj = val_ratio / (val_ratio + test_ratio)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, train_size=val_ratio_adj, stratify=temp_df['label'], random_state=42\n",
    "    )\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    for df, split in zip([train_df, val_df, test_df], [\"Train\", \"Validation\", \"Test\"]):\n",
    "        logger.info(f\"{split} Dataset: {len(df)} images\")\n",
    "        logger.info(f\"Class distribution:\\n{df['class_name'].value_counts()}\")\n",
    "\n",
    "    return train_df, val_df, test_df, class_to_idx\n",
    "\n",
    "# Step 2: Resample Train DataFrame\n",
    "def resample_train_df(train_df, class_to_idx, save_dir):\n",
    "    \"\"\"\n",
    "    Balance the training DataFrame by oversampling minority classes.\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training DataFrame.\n",
    "        class_to_idx (dict): Class name to index mapping.\n",
    "        save_dir (str): Directory to save plots.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Balanced training DataFrame.\n",
    "    \"\"\"\n",
    "    class_counts = train_df['label'].value_counts()\n",
    "    majority_count = class_counts.max()\n",
    "    majority_label = class_counts.idxmax()\n",
    "\n",
    "    dfs_by_class = [train_df[train_df['label'] == label] for label in class_counts.index]\n",
    "    balanced_dfs = []\n",
    "    for df_class, label in zip(dfs_by_class, class_counts.index):\n",
    "        if label == majority_label:\n",
    "            balanced_dfs.append(df_class)\n",
    "        else:\n",
    "            df_oversampled = resample(\n",
    "                df_class, replace=True, n_samples=majority_count, random_state=42\n",
    "            )\n",
    "            balanced_dfs.append(df_oversampled)\n",
    "\n",
    "    train_df_balanced = pd.concat(balanced_dfs)\n",
    "    train_df_balanced = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    logger.info(\"New class distribution after oversampling:\")\n",
    "    logger.info(f\"{train_df_balanced['label'].value_counts()}\")\n",
    "\n",
    "    # Plot class distribution\n",
    "    class_names = sorted(class_to_idx, key=class_to_idx.get)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    counts = train_df_balanced['label'].value_counts().sort_index()\n",
    "    plt.bar(counts.index, counts.values)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Balanced Train Set Class Distribution')\n",
    "    plt.xticks(counts.index, class_names, rotation=45)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, \"train_balanced_class_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return train_df_balanced\n",
    "\n",
    "# Step 3: Data Transformations\n",
    "def get_transforms(split):\n",
    "    \"\"\"\n",
    "    Define Albumentations transforms using ImageNet mean and std.\n",
    "    \n",
    "    Args:\n",
    "        split (str): Dataset split ('train', 'val', 'test').\n",
    "    \n",
    "    Returns:\n",
    "        A.Compose: Transformation pipeline.\n",
    "    \"\"\"\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    if split == \"train\":\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    return A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        # Clamp pixel values to [0, 255] to prevent invalid values\n",
    "        img = np.clip(img, 0, 255)\n",
    "        augmented = self.transform(image=img)\n",
    "        return augmented['image']\n",
    "\n",
    "# Step 4: Custom Dataset\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(self.df['class_name'].unique())\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.df = self._validate_and_filter_dataset()\n",
    "\n",
    "    def _validate_and_filter_dataset(self):\n",
    "        valid_rows = []\n",
    "        for idx in range(len(self.df)):\n",
    "            row = self.df.iloc[idx]\n",
    "            img_path = row['file_path']\n",
    "            label = row['label']\n",
    "            if not os.path.isfile(img_path):\n",
    "                logger.warning(f\"Invalid file path at index {idx}: {img_path}\")\n",
    "                continue\n",
    "            if not isinstance(label, (int, np.integer)) or label not in range(len(self.classes)):\n",
    "                logger.warning(f\"Invalid label at index {idx}: {label}\")\n",
    "                continue\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_array = np.array(img)\n",
    "                if np.isnan(img_array).any() or np.isinf(img_array).any():\n",
    "                    logger.warning(f\"NaN/Inf pixels in image at index {idx}: {img_path}\")\n",
    "                    img.close()\n",
    "                    continue\n",
    "                if img_array.max() > 255 or img_array.min() < 0:\n",
    "                    logger.warning(f\"Out-of-range pixels in image at index {idx}: {img_path}\")\n",
    "                img.close()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Corrupted image at index {idx}: {img_path}, error: {e}\")\n",
    "                continue\n",
    "            valid_rows.append(idx)\n",
    "        if not valid_rows:\n",
    "            raise ValueError(\"No valid items in dataset after filtering\")\n",
    "        filtered_df = self.df.iloc[valid_rows].reset_index(drop=True)\n",
    "        logger.info(f\"Filtered out {len(self.df) - len(filtered_df)} invalid rows\")\n",
    "        return filtered_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['file_path']\n",
    "        label = row['label']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            if torch.isnan(img).any() or torch.isinf(img).any():\n",
    "                logger.error(f\"NaN/Inf in transformed image at index {idx}: {img_path}\")\n",
    "                return None\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return img, label_tensor\n",
    "\n",
    "# Step 5: Create DataLoaders\n",
    "def create_data_loaders(train_df, val_df, test_df, batch_size=64):\n",
    "    \"\"\"\n",
    "    Create DataLoaders for the dataset using ImageNet normalization.\n",
    "    \n",
    "    Args:\n",
    "        train_df, val_df, test_df (pd.DataFrame): DataFrames for each split.\n",
    "        batch_size (int): Batch size.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader, num_classes)\n",
    "    \"\"\"\n",
    "    train_transform = AlbumentationsTransform(get_transforms(\"train\"))\n",
    "    val_test_transform = AlbumentationsTransform(get_transforms(\"val\"))\n",
    "\n",
    "    train_dataset = DataFrameDataset(train_df, transform=train_transform)\n",
    "    val_dataset = DataFrameDataset(val_df, transform=val_test_transform)\n",
    "    test_dataset = DataFrameDataset(test_df, transform=val_test_transform)\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Number of classes: {num_classes}\")\n",
    "    logger.info(f\"Training dataset size: {len(train_loader.dataset)}\")\n",
    "    logger.info(f\"Validation dataset size: {len(val_loader.dataset)}\")\n",
    "    logger.info(f\"Test dataset size: {len(test_loader.dataset)}\")\n",
    "    return train_loader, val_loader, test_loader, num_classes\n",
    "\n",
    "# Step 6: Model Loader\n",
    "def get_model(model_name, num_classes, device):\n",
    "    \"\"\"\n",
    "    Load a pre-trained ResNet18 model for full fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): 'resnet18'.\n",
    "        num_classes (int): Number of classes (3).\n",
    "        device (torch.device): Device to run the model.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: Configured model.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model: {model_name} with {num_classes} classes on {device}\")\n",
    "    if model_name == \"resnet18\":\n",
    "        model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        nn.init.kaiming_normal_(model.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if model.fc.bias is not None:\n",
    "            nn.init.constant_(model.fc.bias, 0)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported.\")\n",
    "    return model.to(device)\n",
    "\n",
    "# Step 7: Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=True, save_dir=None):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_epoch = None\n",
    "        self.early_stop = False\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def __call__(self, val_loss, epoch, model_weights, model_name_prefix):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            self.save_best_weights(model_weights, model_name_prefix)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    logger.info(f\"Early stopping triggered after {self.counter} epochs.\")\n",
    "\n",
    "    def save_best_weights(self, model_weights, model_name_prefix):\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        model_path = os.path.join(self.save_dir, f\"{model_name_prefix}_epoch_{self.best_epoch + 1}.pth\")\n",
    "        torch.save(model_weights, model_path)\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Best model weights saved to {model_path}\")\n",
    "\n",
    "# Step 8: Training and Validation\n",
    "def train_and_validate(model, train_loader, val_loader, optimizer, scheduler, model_name_prefix, \n",
    "                       epochs=25, device=None, early_stopping=None, save_dir=None):\n",
    "    \"\"\"\n",
    "    Train and validate the model with cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        model, train_loader, val_loader, optimizer, scheduler, model_name_prefix, epochs,\n",
    "        device, early_stopping, save_dir\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: Trained model.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            if inputs is None or labels is None:\n",
    "                logger.debug(f\"Skipping invalid training batch {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.clamp(outputs, min=-100, max=100)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                logger.error(f\"NaN/Inf loss at batch {batch_idx}: loss={loss.item()}\")\n",
    "                logger.debug(f\"Input range: {inputs.min().item():.4f} to {inputs.max().item():.4f}\")\n",
    "                logger.debug(f\"Output range: {outputs.min().item():.4f} to {outputs.max().item():.4f}\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.info(f\"Batch {batch_idx}: loss={loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        logger.info(f\"Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                if inputs is None or labels is None:\n",
    "                    continue\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                outputs = torch.clamp(outputs, min=-100, max=100)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_preds += (preds == labels).sum().item()\n",
    "                total_preds += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        valid_losses.append(epoch_loss)\n",
    "        valid_accuracies.append(epoch_acc)\n",
    "        logger.info(f\"Validation Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "        scheduler.step(epoch_loss)\n",
    "        if early_stopping:\n",
    "            early_stopping(epoch_loss, epoch, model.state_dict(), model_name_prefix)\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if early_stopping and early_stopping.best_epoch is not None:\n",
    "        best_model_path = os.path.join(save_dir, f\"{model_name_prefix}_epoch_{early_stopping.best_epoch + 1}.pth\")\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        logger.info(f\"Loaded best model from epoch {early_stopping.best_epoch + 1}\")\n",
    "\n",
    "    # Plot losses and accuracies\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(valid_losses) + 1), valid_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Acc')\n",
    "    plt.plot(range(1, len(valid_accuracies) + 1), valid_accuracies, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'metrics.png'))\n",
    "    plt.close()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Step 9: Testing\n",
    "def test_model(model, test_loader, device, save_dir):\n",
    "    \"\"\"\n",
    "    Test the model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model, test_loader, device, save_dir\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if inputs is None or labels is None:\n",
    "                continue\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.clamp(outputs, min=-100, max=100)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_accuracy = correct_preds / total_preds\n",
    "    logger.info(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Metrics\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    auc_scores = {}\n",
    "    for i, class_name in enumerate(test_loader.dataset.classes):\n",
    "        binary_labels = (all_labels == i).astype(int)\n",
    "        auc_scores[class_name] = roc_auc_score(binary_labels, all_probs[:, i])\n",
    "    logger.info(f\"Test AUC-ROC Scores: {auc_scores}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=test_loader.dataset.classes,\n",
    "                yticklabels=test_loader.dataset.classes)\n",
    "    plt.title(\"Test Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(os.path.join(save_dir, \"test_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                  target_names=test_loader.dataset.classes, digits=4)\n",
    "    with open(os.path.join(save_dir, \"test_classification_report.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "    logger.info(f\"Test Classification Report saved to {save_dir}/test_classification_report.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da3d5174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 14:32:16,868 - INFO - Directories found in O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\dataset\\archive\\SMIDS: ['Abnormal_Sperm', 'Non-Sperm', 'Normal_Sperm']\n",
      "2025-05-31 14:32:16,890 - INFO - Train Dataset: 2100 images\n",
      "2025-05-31 14:32:16,891 - INFO - Class distribution:\n",
      "class_name\n",
      "Normal_Sperm      715\n",
      "Abnormal_Sperm    703\n",
      "Non-Sperm         682\n",
      "Name: count, dtype: int64\n",
      "2025-05-31 14:32:16,891 - INFO - Validation Dataset: 450 images\n",
      "2025-05-31 14:32:16,893 - INFO - Class distribution:\n",
      "class_name\n",
      "Normal_Sperm      153\n",
      "Abnormal_Sperm    151\n",
      "Non-Sperm         146\n",
      "Name: count, dtype: int64\n",
      "2025-05-31 14:32:16,893 - INFO - Test Dataset: 450 images\n",
      "2025-05-31 14:32:16,895 - INFO - Class distribution:\n",
      "class_name\n",
      "Normal_Sperm      153\n",
      "Abnormal_Sperm    151\n",
      "Non-Sperm         146\n",
      "Name: count, dtype: int64\n",
      "2025-05-31 14:32:16,903 - INFO - New class distribution after oversampling:\n",
      "2025-05-31 14:32:16,906 - INFO - label\n",
      "0    715\n",
      "1    715\n",
      "2    715\n",
      "Name: count, dtype: int64\n",
      "2025-05-31 14:32:18,538 - INFO - Filtered out 0 invalid rows\n",
      "2025-05-31 14:32:18,889 - INFO - Filtered out 0 invalid rows\n",
      "2025-05-31 14:32:19,249 - INFO - Filtered out 0 invalid rows\n",
      "2025-05-31 14:32:19,250 - INFO - Number of classes: 3\n",
      "2025-05-31 14:32:19,250 - INFO - Training dataset size: 2145\n",
      "2025-05-31 14:32:19,251 - INFO - Validation dataset size: 450\n",
      "2025-05-31 14:32:19,251 - INFO - Test dataset size: 450\n",
      "2025-05-31 14:32:19,253 - INFO - Loading model: resnet18 with 3 classes on cuda\n",
      "2025-05-31 14:32:19,442 - INFO - Epoch 1/25\n",
      "2025-05-31 14:32:21,085 - INFO - Batch 0: loss=10.0827\n",
      "2025-05-31 14:32:31,782 - INFO - Batch 10: loss=12.0859\n",
      "2025-05-31 14:32:42,536 - INFO - Batch 20: loss=6.8897\n",
      "2025-05-31 14:32:53,307 - INFO - Batch 30: loss=7.0251\n",
      "2025-05-31 14:32:55,701 - INFO - Training Loss: 8.1265, Accuracy: 0.5189\n",
      "2025-05-31 14:32:57,017 - INFO - Validation Loss: 5.3001, Accuracy: 0.6244\n",
      "2025-05-31 14:32:57,113 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_1.pth\n",
      "2025-05-31 14:32:57,160 - INFO - Epoch 2/25\n",
      "2025-05-31 14:32:58,323 - INFO - Batch 0: loss=5.5018\n",
      "2025-05-31 14:33:13,040 - INFO - Batch 10: loss=3.6552\n",
      "2025-05-31 14:33:27,628 - INFO - Batch 20: loss=5.4595\n",
      "2025-05-31 14:33:42,069 - INFO - Batch 30: loss=3.2059\n",
      "2025-05-31 14:33:45,416 - INFO - Training Loss: 3.7818, Accuracy: 0.7203\n",
      "2025-05-31 14:33:47,340 - INFO - Validation Loss: 4.1962, Accuracy: 0.6889\n",
      "2025-05-31 14:33:47,471 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_2.pth\n",
      "2025-05-31 14:33:47,563 - INFO - Epoch 3/25\n",
      "2025-05-31 14:33:48,760 - INFO - Batch 0: loss=1.8942\n",
      "2025-05-31 14:33:58,968 - INFO - Batch 10: loss=2.4809\n",
      "2025-05-31 14:34:08,791 - INFO - Batch 20: loss=1.2677\n",
      "2025-05-31 14:34:18,871 - INFO - Batch 30: loss=1.9461\n",
      "2025-05-31 14:34:21,131 - INFO - Training Loss: 2.3075, Accuracy: 0.7744\n",
      "2025-05-31 14:34:23,436 - INFO - Validation Loss: 3.3811, Accuracy: 0.7311\n",
      "2025-05-31 14:34:23,496 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_3.pth\n",
      "2025-05-31 14:34:23,566 - INFO - Epoch 4/25\n",
      "2025-05-31 14:34:24,697 - INFO - Batch 0: loss=1.2302\n",
      "2025-05-31 14:34:37,106 - INFO - Batch 10: loss=1.4604\n",
      "2025-05-31 14:34:49,287 - INFO - Batch 20: loss=1.8273\n",
      "2025-05-31 14:35:01,572 - INFO - Batch 30: loss=1.2116\n",
      "2025-05-31 14:35:04,435 - INFO - Training Loss: 1.4448, Accuracy: 0.8200\n",
      "2025-05-31 14:35:05,765 - INFO - Validation Loss: 2.9018, Accuracy: 0.7556\n",
      "2025-05-31 14:35:05,867 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_4.pth\n",
      "2025-05-31 14:35:05,963 - INFO - Epoch 5/25\n",
      "2025-05-31 14:35:07,117 - INFO - Batch 0: loss=2.3661\n",
      "2025-05-31 14:35:19,526 - INFO - Batch 10: loss=0.7645\n",
      "2025-05-31 14:35:32,156 - INFO - Batch 20: loss=0.6180\n",
      "2025-05-31 14:35:44,949 - INFO - Batch 30: loss=1.0606\n",
      "2025-05-31 14:35:47,985 - INFO - Training Loss: 1.1934, Accuracy: 0.8583\n",
      "2025-05-31 14:35:49,580 - INFO - Validation Loss: 2.7433, Accuracy: 0.7556\n",
      "2025-05-31 14:35:49,660 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_5.pth\n",
      "2025-05-31 14:35:49,745 - INFO - Epoch 6/25\n",
      "2025-05-31 14:35:50,950 - INFO - Batch 0: loss=1.1513\n",
      "2025-05-31 14:36:03,827 - INFO - Batch 10: loss=1.9386\n",
      "2025-05-31 14:36:16,676 - INFO - Batch 20: loss=1.7641\n",
      "2025-05-31 14:36:28,932 - INFO - Batch 30: loss=1.0220\n",
      "2025-05-31 14:36:31,792 - INFO - Training Loss: 0.9693, Accuracy: 0.8723\n",
      "2025-05-31 14:36:33,029 - INFO - Validation Loss: 2.4521, Accuracy: 0.7422\n",
      "2025-05-31 14:36:33,096 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_6.pth\n",
      "2025-05-31 14:36:33,180 - INFO - Epoch 7/25\n",
      "2025-05-31 14:36:34,263 - INFO - Batch 0: loss=0.6067\n",
      "2025-05-31 14:36:46,519 - INFO - Batch 10: loss=0.4673\n",
      "2025-05-31 14:36:58,933 - INFO - Batch 20: loss=0.6622\n",
      "2025-05-31 14:37:11,427 - INFO - Batch 30: loss=0.8277\n",
      "2025-05-31 14:37:14,333 - INFO - Training Loss: 0.7435, Accuracy: 0.9016\n",
      "2025-05-31 14:37:15,695 - INFO - Validation Loss: 2.3973, Accuracy: 0.7511\n",
      "2025-05-31 14:37:15,761 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_7.pth\n",
      "2025-05-31 14:37:15,823 - INFO - Epoch 8/25\n",
      "2025-05-31 14:37:17,083 - INFO - Batch 0: loss=0.7564\n",
      "2025-05-31 14:37:29,377 - INFO - Batch 10: loss=0.2811\n",
      "2025-05-31 14:37:41,614 - INFO - Batch 20: loss=0.0387\n",
      "2025-05-31 14:37:53,972 - INFO - Batch 30: loss=0.7131\n",
      "2025-05-31 14:37:56,822 - INFO - Training Loss: 0.5797, Accuracy: 0.9124\n",
      "2025-05-31 14:37:58,173 - INFO - Validation Loss: 2.1510, Accuracy: 0.7933\n",
      "2025-05-31 14:37:58,233 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_8.pth\n",
      "2025-05-31 14:37:58,306 - INFO - Epoch 9/25\n",
      "2025-05-31 14:37:59,410 - INFO - Batch 0: loss=0.4152\n",
      "2025-05-31 14:38:11,832 - INFO - Batch 10: loss=0.2698\n",
      "2025-05-31 14:38:24,144 - INFO - Batch 20: loss=0.2820\n",
      "2025-05-31 14:38:36,518 - INFO - Batch 30: loss=0.1181\n",
      "2025-05-31 14:38:39,511 - INFO - Training Loss: 0.5633, Accuracy: 0.9142\n",
      "2025-05-31 14:38:41,234 - INFO - Validation Loss: 2.0041, Accuracy: 0.7911\n",
      "2025-05-31 14:38:41,312 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_9.pth\n",
      "2025-05-31 14:38:41,393 - INFO - Epoch 10/25\n",
      "2025-05-31 14:38:42,604 - INFO - Batch 0: loss=0.5718\n",
      "2025-05-31 14:38:55,446 - INFO - Batch 10: loss=0.6416\n",
      "2025-05-31 14:39:08,192 - INFO - Batch 20: loss=0.1394\n",
      "2025-05-31 14:39:20,866 - INFO - Batch 30: loss=0.2175\n",
      "2025-05-31 14:39:23,850 - INFO - Training Loss: 0.4993, Accuracy: 0.9235\n",
      "2025-05-31 14:39:25,436 - INFO - Validation Loss: 2.1489, Accuracy: 0.7711\n",
      "2025-05-31 14:39:25,538 - INFO - Epoch 11/25\n",
      "2025-05-31 14:39:26,754 - INFO - Batch 0: loss=0.2576\n",
      "2025-05-31 14:39:39,810 - INFO - Batch 10: loss=0.3540\n",
      "2025-05-31 14:39:52,664 - INFO - Batch 20: loss=0.4464\n",
      "2025-05-31 14:40:04,840 - INFO - Batch 30: loss=0.2232\n",
      "2025-05-31 14:40:07,764 - INFO - Training Loss: 0.3861, Accuracy: 0.9375\n",
      "2025-05-31 14:40:09,434 - INFO - Validation Loss: 1.9296, Accuracy: 0.7956\n",
      "2025-05-31 14:40:09,564 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_11.pth\n",
      "2025-05-31 14:40:09,640 - INFO - Epoch 12/25\n",
      "2025-05-31 14:40:10,840 - INFO - Batch 0: loss=0.1243\n",
      "2025-05-31 14:40:23,359 - INFO - Batch 10: loss=0.8165\n",
      "2025-05-31 14:40:35,859 - INFO - Batch 20: loss=0.4551\n",
      "2025-05-31 14:40:48,219 - INFO - Batch 30: loss=0.4889\n",
      "2025-05-31 14:40:51,215 - INFO - Training Loss: 0.2961, Accuracy: 0.9427\n",
      "2025-05-31 14:40:52,870 - INFO - Validation Loss: 2.1412, Accuracy: 0.7911\n",
      "2025-05-31 14:40:52,959 - INFO - Epoch 13/25\n",
      "2025-05-31 14:40:54,119 - INFO - Batch 0: loss=0.7762\n",
      "2025-05-31 14:41:06,503 - INFO - Batch 10: loss=0.5157\n",
      "2025-05-31 14:41:18,858 - INFO - Batch 20: loss=0.0622\n",
      "2025-05-31 14:41:31,548 - INFO - Batch 30: loss=0.3921\n",
      "2025-05-31 14:41:34,543 - INFO - Training Loss: 0.3486, Accuracy: 0.9417\n",
      "2025-05-31 14:41:36,224 - INFO - Validation Loss: 1.9949, Accuracy: 0.7911\n",
      "2025-05-31 14:41:36,304 - INFO - Epoch 14/25\n",
      "2025-05-31 14:41:37,481 - INFO - Batch 0: loss=0.3868\n",
      "2025-05-31 14:41:49,879 - INFO - Batch 10: loss=0.5013\n",
      "2025-05-31 14:42:02,170 - INFO - Batch 20: loss=0.1430\n",
      "2025-05-31 14:42:14,459 - INFO - Batch 30: loss=0.5475\n",
      "2025-05-31 14:42:17,345 - INFO - Training Loss: 0.3121, Accuracy: 0.9427\n",
      "2025-05-31 14:42:18,630 - INFO - Validation Loss: 2.0529, Accuracy: 0.7867\n",
      "2025-05-31 14:42:18,722 - INFO - Epoch 15/25\n",
      "2025-05-31 14:42:19,807 - INFO - Batch 0: loss=0.0233\n",
      "2025-05-31 14:42:32,106 - INFO - Batch 10: loss=0.1753\n",
      "2025-05-31 14:42:44,181 - INFO - Batch 20: loss=0.3502\n",
      "2025-05-31 14:42:56,410 - INFO - Batch 30: loss=0.1087\n",
      "2025-05-31 14:42:59,432 - INFO - Training Loss: 0.2461, Accuracy: 0.9510\n",
      "2025-05-31 14:43:01,102 - INFO - Validation Loss: 2.1202, Accuracy: 0.7867\n",
      "2025-05-31 14:43:01,201 - INFO - Epoch 16/25\n",
      "2025-05-31 14:43:02,368 - INFO - Batch 0: loss=0.3057\n",
      "2025-05-31 14:43:15,193 - INFO - Batch 10: loss=0.0517\n",
      "2025-05-31 14:43:27,568 - INFO - Batch 20: loss=0.0859\n",
      "2025-05-31 14:43:39,749 - INFO - Batch 30: loss=0.3512\n",
      "2025-05-31 14:43:42,621 - INFO - Training Loss: 0.2446, Accuracy: 0.9562\n",
      "2025-05-31 14:43:43,930 - INFO - Validation Loss: 1.9287, Accuracy: 0.7889\n",
      "2025-05-31 14:43:43,995 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_16.pth\n",
      "2025-05-31 14:43:44,050 - INFO - Epoch 17/25\n",
      "2025-05-31 14:43:45,188 - INFO - Batch 0: loss=0.0694\n",
      "2025-05-31 14:43:58,155 - INFO - Batch 10: loss=0.0923\n",
      "2025-05-31 14:44:10,743 - INFO - Batch 20: loss=0.0889\n",
      "2025-05-31 14:44:22,917 - INFO - Batch 30: loss=0.1049\n",
      "2025-05-31 14:44:25,751 - INFO - Training Loss: 0.1675, Accuracy: 0.9618\n",
      "2025-05-31 14:44:26,955 - INFO - Validation Loss: 1.9033, Accuracy: 0.7889\n",
      "2025-05-31 14:44:27,011 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_17.pth\n",
      "2025-05-31 14:44:27,090 - INFO - Epoch 18/25\n",
      "2025-05-31 14:44:28,273 - INFO - Batch 0: loss=0.4215\n",
      "2025-05-31 14:44:40,400 - INFO - Batch 10: loss=0.0942\n",
      "2025-05-31 14:44:52,606 - INFO - Batch 20: loss=0.4267\n",
      "2025-05-31 14:45:05,460 - INFO - Batch 30: loss=0.1923\n",
      "2025-05-31 14:45:08,448 - INFO - Training Loss: 0.2114, Accuracy: 0.9590\n",
      "2025-05-31 14:45:09,981 - INFO - Validation Loss: 1.8234, Accuracy: 0.8111\n",
      "2025-05-31 14:45:10,056 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_18.pth\n",
      "2025-05-31 14:45:10,147 - INFO - Epoch 19/25\n",
      "2025-05-31 14:45:11,363 - INFO - Batch 0: loss=0.0875\n",
      "2025-05-31 14:45:24,095 - INFO - Batch 10: loss=0.4122\n",
      "2025-05-31 14:45:36,684 - INFO - Batch 20: loss=0.1692\n",
      "2025-05-31 14:45:49,169 - INFO - Batch 30: loss=0.0119\n",
      "2025-05-31 14:45:51,995 - INFO - Training Loss: 0.1082, Accuracy: 0.9781\n",
      "2025-05-31 14:45:53,230 - INFO - Validation Loss: 1.8577, Accuracy: 0.8022\n",
      "2025-05-31 14:45:53,295 - INFO - Epoch 20/25\n",
      "2025-05-31 14:45:54,456 - INFO - Batch 0: loss=0.0364\n",
      "2025-05-31 14:46:07,044 - INFO - Batch 10: loss=0.0702\n",
      "2025-05-31 14:46:19,802 - INFO - Batch 20: loss=0.0004\n",
      "2025-05-31 14:46:32,166 - INFO - Batch 30: loss=0.1827\n",
      "2025-05-31 14:46:35,051 - INFO - Training Loss: 0.1881, Accuracy: 0.9641\n",
      "2025-05-31 14:46:36,235 - INFO - Validation Loss: 1.8400, Accuracy: 0.7956\n",
      "2025-05-31 14:46:36,295 - INFO - Epoch 21/25\n",
      "2025-05-31 14:46:37,450 - INFO - Batch 0: loss=0.4217\n",
      "2025-05-31 14:46:49,517 - INFO - Batch 10: loss=0.1448\n",
      "2025-05-31 14:47:01,597 - INFO - Batch 20: loss=0.2214\n",
      "2025-05-31 14:47:14,439 - INFO - Batch 30: loss=0.3393\n",
      "2025-05-31 14:47:17,416 - INFO - Training Loss: 0.1337, Accuracy: 0.9716\n",
      "2025-05-31 14:47:18,982 - INFO - Validation Loss: 1.6962, Accuracy: 0.8222\n",
      "2025-05-31 14:47:19,072 - INFO - Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\\resnet18_sperm_epoch_21.pth\n",
      "2025-05-31 14:47:19,123 - INFO - Epoch 22/25\n",
      "2025-05-31 14:47:20,450 - INFO - Batch 0: loss=0.5234\n",
      "2025-05-31 14:47:33,598 - INFO - Batch 10: loss=0.0440\n",
      "2025-05-31 14:47:46,459 - INFO - Batch 20: loss=0.1065\n",
      "2025-05-31 14:47:58,980 - INFO - Batch 30: loss=0.0027\n",
      "2025-05-31 14:48:01,880 - INFO - Training Loss: 0.1645, Accuracy: 0.9646\n",
      "2025-05-31 14:48:04,013 - INFO - Validation Loss: 1.7275, Accuracy: 0.8178\n",
      "2025-05-31 14:48:04,103 - INFO - Epoch 23/25\n",
      "2025-05-31 14:48:05,382 - INFO - Batch 0: loss=0.0035\n",
      "2025-05-31 14:48:17,732 - INFO - Batch 10: loss=0.0033\n",
      "2025-05-31 14:48:29,906 - INFO - Batch 20: loss=0.2026\n",
      "2025-05-31 14:48:42,244 - INFO - Batch 30: loss=0.0014\n",
      "2025-05-31 14:48:45,203 - INFO - Training Loss: 0.1088, Accuracy: 0.9758\n",
      "2025-05-31 14:48:46,746 - INFO - Validation Loss: 1.8127, Accuracy: 0.7956\n",
      "2025-05-31 14:48:46,840 - INFO - Epoch 24/25\n",
      "2025-05-31 14:48:48,064 - INFO - Batch 0: loss=0.0812\n",
      "2025-05-31 14:49:00,566 - INFO - Batch 10: loss=0.3659\n",
      "2025-05-31 14:49:12,714 - INFO - Batch 20: loss=0.2723\n",
      "2025-05-31 14:49:24,807 - INFO - Batch 30: loss=0.2161\n",
      "2025-05-31 14:49:27,665 - INFO - Training Loss: 0.1144, Accuracy: 0.9790\n",
      "2025-05-31 14:49:28,973 - INFO - Validation Loss: 1.9607, Accuracy: 0.7978\n",
      "2025-05-31 14:49:29,043 - INFO - Epoch 25/25\n",
      "2025-05-31 14:49:30,172 - INFO - Batch 0: loss=0.2472\n",
      "2025-05-31 14:49:42,624 - INFO - Batch 10: loss=0.0041\n",
      "2025-05-31 14:49:55,110 - INFO - Batch 20: loss=0.2592\n",
      "2025-05-31 14:50:07,666 - INFO - Batch 30: loss=0.1053\n",
      "2025-05-31 14:50:10,606 - INFO - Training Loss: 0.1066, Accuracy: 0.9720\n",
      "2025-05-31 14:50:12,099 - INFO - Validation Loss: 1.9598, Accuracy: 0.8111\n",
      "2025-05-31 14:50:12,253 - INFO - Loaded best model from epoch 21\n",
      "2025-05-31 14:50:14,164 - INFO - Test Loss: 1.8413, Test Accuracy: 0.8022\n",
      "2025-05-31 14:50:14,179 - INFO - Test AUC-ROC Scores: {'Abnormal_Sperm': 0.8954461892843695, 'Non-Sperm': 0.985580389329488, 'Normal_Sperm': 0.9245175062168526}\n",
      "2025-05-31 14:50:14,344 - INFO - Test Classification Report saved to O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots/test_classification_report.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = r\"O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\dataset\\archive\\SMIDS\"\n",
    "    save_dir = r\"O:\\O drive\\AI\\my project\\medical image projects\\Sperm Morphology Image Data Set (SMIDS)\\plots\"\n",
    "\n",
    "    # Data preparation\n",
    "    train_df, val_df, test_df, class_to_idx = create_dataframes(root_dir)\n",
    "    train_df_balanced = resample_train_df(train_df, class_to_idx, save_dir)\n",
    "    train_loader, val_loader, test_loader, num_classes = create_data_loaders(\n",
    "        train_df_balanced, val_df, test_df, batch_size=64\n",
    "    )\n",
    "\n",
    "    # Model setup\n",
    "    model = get_model(\"resnet18\", num_classes, device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True, save_dir=save_dir)\n",
    "    model_name_prefix = \"resnet18_sperm\"\n",
    "\n",
    "    # Train and test\n",
    "    model = train_and_validate(\n",
    "        model, train_loader, val_loader, optimizer, scheduler,\n",
    "        model_name_prefix, epochs=25, device=device, early_stopping=early_stopping,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "    test_model(model, test_loader, device, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361482bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
